# Machine learning {#four}

This week you will learn about machine learning for classification using _R_. Objectives are:

- To provide an overview of the underlying concepts of
machine learning for classification.

- To provide an introduction to some popular
classification algorithms.

- To explore these classification algorithms using various packages in _R_.

- To choose a classification algorithm and apply it to the statewide COVID-19 dataset.

## Day 22 (Monday) Zoom check-in

Here is an overview of what we'll cover in today's Zoom session:

- Overview of Machine Learning for Classification (30 minutes)

- Introduction to the Random Forest algorithm (5 minutes)

- A Random Forest Example in _R_ using COVID-19 Data (25 minutes)

### A Machine Learning Primer

Machine Learning (ML) may be defined as using computers to make inferences about data.

- A mapping of inputs to outputs, Y = f(<b>X</b> , <b>β</b>)

- <i>ML for Classification</i> refers to algorithms that map inputs to a discrete set of outputs (i.e. classes or categories)

  - For example, predicting health risk (mild, moderate, severe) based on patient data (height, weight, age, smoker?)
  
  - Or predicting [pandemic severity index][] (PSI) of COVID-19 in a state based on statewide population data.

```{r echo=FALSE}
cats=c(1:5)
frate=c("< 0.1%  ", "0.1% - 0.5%  ", "0.5% - 1.0%  ", "1.0% - 2.0%  ", "> 2.0%  ")
example=c("seasonal flu", "Asian flu", "n/a", "n/a", "Spanish flu")
df = data.frame(PSI=cats, Death.Rate=frate, Example=example)
print(df, row.names = FALSE, right = FALSE)
```

  – Predictions are typically expressed as a vector of probabilities.
  
    – e.g. Pr(cancerous) vs. Pr(benign)
    
    - e.g. Pr(PSI=1), Pr(PSI=2), ..., Pr(PSI=5)

#### A "Black Box" View of Machine Learning {-}
The diagram below illustrates the concept in terms of a "black box" model that converts inputs into predictions.

```{r echo = FALSE, fig.align="center", out.width="50%"}
knitr::include_graphics('images/ml_black_box_generic.png')
```

#### A "Black Box" Example {-}

The diagram below illustrates a "black box" model that converts height, weight, and age into a prediction of whether or not the individual is male or female.

```{r echo = FALSE, fig.align="center", out.width="50%"}
knitr::include_graphics('images/ml_black_box_example.png')
```

#### Some Important Machine Learning Terms {-}

The above example highlights some important machine learning terminology:

- <b>Features</b> (<b>X</b>): the inputs (also known as descriptive attributes or explanatory variables) to the algorithm.

- <b>Parameters</b> (<b>$\beta$</b>): these are internal variables of the selected machine learning algorithm and are also known as coefficients or training weights. Algorithm parameters need to be adjusted to minimize the deviation between predictions and observations.

- <b>Labels</b> (<b>Y</b>): these are the outputs of the algorithm (i.e. the categories you are attempting to predict).

- <b>Training Data</b>: this is a data sets containing paired observations of inputs (i.e features) and outputs (i.e. labels). Training data is also known as measurement data, observation data or calibration data.

- <b>Training</b>: this is the process of adjusting algorithm paramters (<b>$\beta$</b>) to obtain the best possible match between training data and corresponding model predictions. Training is also known as model calibration or parameter estimations.

An example set of training data that could be used in the gender prediction example is given below:

```{r echo=FALSE}
h=c(1.69,1.74,1.92,1.80,1.59,1.85,1.75,1.96,1.85,1.78,1.74,1.81,1.73)
w=c(62,76,82,100,47,75,63,83,39,58,70,57,78)
a=c(30,27,25,41,24,26,33,33,32,28,30,26,32)
g=c("male","female","male","male","female","male","female","male","male","female","female","female","male")

df = data.frame(height_m=h,weight_kg=w,age_y=a,gender=g)
print(df, row.names = FALSE, right = FALSE)
```

Upon completion of training, it is important to evaluate the quality of the model and it's skill or ability at making correct predictions. Some terms related to this evaluation process are defined below:

- <b>Classification Accuracy</b>: this is the ratio of correct predictions to the total predictions when validating a trained classification algorithm.

- <b>Confusion Matrix</b>: this is a more detailed summary (relative to classification accuracy) of the performance of a classification algorithm. The diagonals of the matrix count how often the algorithm yielded the correct classification for each class. The off-diagonal entries count how often the algorithm confused one class with another.

The figure below illustrates the classification accuracy and confusion matrix for an example that attempts to classify images of fruits.

```{r echo = FALSE, fig.align="center", out.width="50%"}
knitr::include_graphics('images/ml_ca_fruits_example.png')
```

- <b> ROC curve</b>: As illustrated in the figure below, the ROC (Receiver Operating Characteristic) curve plots the true positive (TP) vs. false positive (FP) rate at various probability thresholds. In the figure, the dashed blue line represents a hypothetical ROC curve for some machine learning model and the solid red line is the curve for a "non-informative" model (i.e. a model that makes a uniform random guess). As such, we'd like the blue curve to be as far above the red curve as possible.

```{r echo = FALSE, fig.align="center", , out.width="75%"}
knitr::include_graphics('images/ml_roc_curve_example.png')
```

- <b>AUC</b>: this stands for "area under curve" and is the area under the ROC curve. In the previous figure, the AUC would be the area under the dashed blue curve. Values of AUC quantify the degree to which an ROC curve lies above (or below) the red "non-informative" curve. Some interesting AUC values:

  - AUC = 0.0: the model is always wrong (with respect to TP vs. FP)
  
  - AUC = 0.5: the model is no better than guessing (i.e. the model matches the red "non-informative" curve in the figure)
  
  - AUC = 1.0: the model is always right (with respect to TP vs. FP)

#### The Machine Learning Process {-}

Now that we've defined some of the important machine learning terminology let's take a 30,000 foot view at the overall machine learning process. This is a general description of the process that you'll want to follow each time you build and use a machine learning model. The process is illustrated in the figure below (with credit to [Jinesh Maloo][]):

```{r echo = FALSE, fig.align="center", out.width="75%"}
knitr::include_graphics('images/ml_process_maloo.png')
```

- Step 1: Prepare labeled data for training and validation.

- Step 2: Select a machine learning algorithm (i.e. model).

- Step 3: Train the model.

- Step 4: Evaluate model performance.

- If model is useful:

  - Step 5: Apply to unlabled data.
  
- Else (needs improvement):

  - Collect more data (go back to Step 1).
  
  - Revise model (go back to Step 2)

[pandemic severity index]: https://en.wikipedia.org/wiki/Pandemic_severity_index

[Jinesh Maloo]: https://blog.usejournal.com/machine-learning-for-beginners-from-zero-level-8be5b89bf77c

### The Random Forest Algorithm

The random forest algorithm is a popular choice for machine learning.

- Over 20 _R_ packages have an implementation of some form of the algorithm.

- We'll be using the `randomForest` pacakge.

- The algorithm is like `bagging` (boostrap aggregating) regression trees, but the rgression trees are de-correlated.

The figure below illustrates one possible `tree` in a `random forest` for a gender prediction model. In computer science terminology, each split in the figure is a `branch` of a graph `tree`. In simple terms, the split points are randomly generated and the resulting `trees` combine to form a `random forest`.

```{r echo = FALSE, fig.align="center", out.width="75%"}
knitr::include_graphics('images/ml_rf_tree_gender.png')
```

The figure below illustrates a set of 6 `trees` that make up a `random forest` for predicting housing prices. The image is courtesy [Bradley Boehmke at the University of Cincinnati][]. Examine the figure closely and notice that:

  - The split variables can differ across trees (not all variables are included in all trees).
  
  - The split variables can differ within trees (not all paths consider the same set of variables).
  
  - The order of splits can differ.
  
  - The split values can differ.
  
```{r echo = FALSE, fig.align="center", out.width="100%"}
knitr::include_graphics('images/ml_rf_tree_housing.png')
```

The job of the random forest algorithm is to determine the optimal set of trees for your data set, including the splitting configuration (i.e. order, values, etc.).

[Bradley Boehmke at the University of Cincinnati]: https://uc-r.github.io/random_forests

### Preparing a COVID-19 Dataset for Machine Learning {-}

We'd like to predict the severity of COVID-19 in a given state using statewide <i>feature</i> data like population, urban density, number of hospital beds, date of stay at home order, etc. We've already seen that we can get the information about cases and deaths from the New York Times github page. However, gathering corresponding statewide feature data requires quite a bit of hunting through various public websites. Consequently, we're going to skip over the painstaking process of marshalling the feature data and just provide you with a dataset that is already nice and prepped for machine learning.  

You'll work with two `.csv` files - a <b>data</b> file that contains a veriety of statewide data, and a <b>metadata</b> file that describes the various columns of the data file. This combination of data and metadata files is a common way of sharing publicly avaialble datasets.

To give you an idea of what was involved in assembling the data and metadate file, a summary of the data collection and processing steps is given below:

- First, a snapshot of the New York Times COVID-19 data from April 27th was downloaded from the [nytimes github repo][] and stored on local disk.

```
$ wget https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv
$ mv us-counties.csv us-counties_04_27_2020.csv
```

- The data was processed using _R_ : 

  - the cases and deaths in `us-counties_04_27_2020.csv` were aggregated into statewide values.
  
  - the death rate was calculated and categorized according to an 8-point severity index
  
  - finally, the statewide data was exported as a `.csv` file

```{r eval=FALSE}

# get data from file
covid_data_file = file.path("C:\\Matott\\MyQuaRantine", "us-counties_04_27_2020.csv")

# read in as data frame
us_data = read.csv(covid_data_file, stringsAsFactors = FALSE)

# aggregate by county and state
cases_county_state <- aggregate( cases ~ county + state, us_data, max )
deaths_county_state <- aggregate( deaths ~ county + state, us_data, max )

# aggregate by state
cases_state <- aggregate(cases ~ state, cases_county_state, sum)
deaths_state <- aggregate(deaths ~ state, deaths_county_state, sum)

# calculate death rate
death_rate_state <- 100.00 * (deaths_state$deaths / cases_state$cases)

# Assign the following severity index using cut:
##  PSI Death.Rate    
##  1   < 0.1%        
##  2   0.1% - 0.5%   
##  3   0.5% - 1.0%   
##  4   1.0% - 2.0%   
##  5   2.0% - 4.0%   
##  6   4.0% - 6.0%   
##  7   6.0% - 8.0%   
##  8   >8.0%         
psi_state <- cut(death_rate_state, 
                 breaks=c(0.0,0.1,0.5,1.0,2.0,4.0,6.0,8.0,100.0),
                 labels=c(1,2,3,4,5,6,7,8))

# combine into new data frame
out_df = data.frame(
    state=cases_state$state, 
    cases=cases_state$cases, 
    deaths=deaths_state$deaths,
    death_rate=death_rate_state,
    severity_index=psi_state)

# write out as csv
my_out_file <- file.path("C:\\Matott\\MyQuaRantine", "covid_data.csv")
write.csv(out_df, file = my_out_file, row.names = FALSE)
```

- The resulting statewide COVID-19 <b>label</b> data (i.e. what we would like to predict) was augmented with 32 statewide features, including population, percent urban, number of hospital beds, etc. Feature data was collected from a veriety of sources, including the Center for Disease Control, the American Heart Association, the U.S. Census Bureau, etc. In some cases the feature data was available for direct download (e.g. as a `.csv` file) and in other cases the feature data was manually harvested (e.g. cut-and-paste from websites).

- The augmented (i.e. features + labels) `.csv` file was split into two `.csv` files that you will need to download:

  - [statewide_covid_19_data_04_27_2020.csv](assets/statewide_covid_19_data_04_27_2020.csv): This file contains the final COVID-19 machine learning data set, but features and labels are coded so that feature columns are named `X01`, `X02`, `X03`, etc. and label colunms are named `Y01`, `Y02`, `Y03`, etc. 

  - [statewide_covid_19_metadata_04_27_2020.csv](assets/statewide_covid_19_metadata_04_27_2020.csv): This file maps the column names in the data file to more meaningful names and desciptions (including units) of the associated variables. For example `X01` is `Pct_Sun` and has a description of `Percent sunny days`. This is known as <b>metadata</b> - data that describes other data.
  
Click on the links above to download the data and metadata files that you'll need for the machine learning examples presented throughout the week.

[nytimes github repo]: https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv

### A Random Forest Example Using COVID-19 Data

Let's apply the random forest algorithm to the COVID-19 dataset. We'll build out the required _R_ code in sections. To get started, open a new _R_ script in _RStudio_ and name it `covid_19_rf.R`. Enter the code below:

```{r}
library(randomForest)
library(readr)

data_file = file.path("C:/Matott/MyQuaRantine",
                      "statewide_covid_19_data_04_27_2020.csv")
df = read_csv(data_file)
# coerce severity to a factor (so RF algorithm uses classification)
df$Y04 = as.factor(df$Y04)
df

metadata_file = file.path("C:/Matott/MyQuaRantine",
                          "statewide_covid_19_metadata_04_27_2020.csv")
mdf = read_csv(metadata_file)
mdf
```

Enter the code above and try to run it. You may get an errors about missing the `randomForest` package. You can install it from the RStudio console (see below) or using the installer in the RStudio `packages` pane.

```{r eval=FALSE}
install.packages("randomForest")
```

Now we've loaded the data and metadata file. Let's pick a subset of 5 of the features and use them to try and predict the pandemic severity index (i.e. `Y04`):

```{r}
library(dplyr)

# describe all possible features and labels
print(mdf, n=nrow(mdf))

# select some features and the PSI label
my_x = c("X01","X10","X12","X13","X23")
my_y = c("Y04")
my_xy = c(my_x, my_y)

# get descriptions of the selected features and label
filter(mdf, Code %in% my_xy)

# subset the dataframe
rf_df = select(df, all_of(my_xy))
```

Now we’ll add code to train a basic Random Forest model:

```{r}
# split into train (75%) and test (25%) datasets
train = rf_df[seq(1,nrow(rf_df), by = 4),]
train = rbind(train, rf_df[seq(2,nrow(rf_df), by = 4),])
train = rbind(train, rf_df[seq(3,nrow(rf_df), by = 4),])
test = rf_df[seq(4,nrow(rf_df), by = 4),]

# create and train the RF model
model = randomForest(Y04 ~ X01 + X10 + X12 + X13 + X23, 
                     data = train)
print(model) # show results, includes confusion matrix
importance(model) # measure of parameter importance
```

Taking a look at the results we see and error rate of >50%. The classification accuracy for the training dataset is `100% - error rate` - so not a very good model. Further we can see some systematic failures in the confusion matrix. The most likely culprit is that the set of features is inadequate for making the desired prediction. We should re-run the model using different or additional features.

It's also good practice to examine the ROC and AUC metrics. For a problem with multiple classes (as opposed to a binary True/False or Yes/No problem) we can compute the ROC curve curve and AUC measures using a “one vs all” approach:

- First, extract predicted probabilities from the RF model (the scores).

- Next, extract actual classification for each category.

- Finally, leverage two commands of the `ROCR` module:

  - `prediction()`: retrieve scores

  - `performance()`: generates TPR, FPR, and AUC measure through two separate calls

Unfortunately, the calculation is fairly involved and uses a looping structure (i.e. a `for` loop) to assemble the plot and auc values:

```{r}
library(ROCR)

# compute ROC/AUC using test dataset
x_test = select(test, all_of(my_x))
y_test = select(test, all_of(my_y))
probs = predict(model, x_test, type='prob')
lvls = unique(as.character(y_test[[1]])) # category names
# up to 8 colors needed
cols = c("red","orange","yellow","green",
         "blue","indigo","violet","black")
for(i in 1:length(lvls)) # for each category
{
  actual = as.numeric(y_test[[1]] == lvls[i])
  score = probs[,i]
  
  pred = prediction(score, actual)
  perf = performance(pred, "tpr", "fpr")
  if(i == 1){
    plot(perf, main="ROC Curve", col=cols[i])
  }
  else {
    plot(perf, main="ROC Curve", col=cols[i], add = TRUE)
  }
  
  # calculate the AUC and print
  auc = performance(pred, measure="auc")
  auc = as.numeric(auc@y.values)
  cat('AUC for PSI of ',lvls[i], ' = ', auc, '\n')
}
legend("bottomright",legend=paste("PSI =",lvls), col=cols, lty=1, 
       cex=1.0) # adjust cex to scale the legend
lines(x=c(0,1),y=c(0,1),lty=2) # add threshold for non-informative 
```

## Day 23

For today's independent work you will learn about the Support Support Vector Machine (SVM) algorithm and apply it to the radiomics data that you worked with on Monday.

## Day 24

For today's independent work you will learn about the K-nearest neighbors (KNN) algorithm and apply it to the radiomics data.

## Day 25

For today's independent work you will learn about artificial neural networks (ANN). You will attempt to add Keras and TensorFlow support to your R/RStudio installation. If this turns out to be too dofficult, you will learn some "remote computing" alternatives that you can use in lieu of a local _R/Kera/TensorFlow_ installation. Finally, you'll apply `R-Keras-TensorFlow` to the radiomics example that you've been working with during the week.  

## Day 26 (Friday) Zoom check-in

### Review and trouble shoot (25 minutes)

### Next week (25 minutes)

## Day 27

## Day 28

Self-directed activities.
